{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7c6e505",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'descriptor' from 'google.protobuf' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "File \u001b[1;32mc:\\Users\\Harshad\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Harshad\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\__init__.py:37\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Harshad\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msix\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m coordination_config_pb2\n",
      "File \u001b[1;32mc:\\Users\\Harshad\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      6\u001b[0m _b\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;01mlambda\u001b[39;00m x:x) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28;01mlambda\u001b[39;00m x:x\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m descriptor \u001b[38;5;28;01mas\u001b[39;00m _descriptor\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m message \u001b[38;5;28;01mas\u001b[39;00m _message\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reflection \u001b[38;5;28;01mas\u001b[39;00m _reflection\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'descriptor' from 'google.protobuf' (unknown location)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, TFGemmaForCausalLM, GemmaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed7ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "MODEL_NAME = \"google/gemma-3n-E2B\" # Or gemma-3n-E4B for a larger model\n",
    "# Check Google's official Gemma 3n resources for recommended image input sizes.\n",
    "# As of current info, Gemma 3 models support 256x256, 512x512, or 768x768.\n",
    "# Let's target a common size for efficiency on device.\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10 # You might need more or less\n",
    "LEARNING_RATE = 2e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bdfcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Path to your dataset (e.g., download from Kaggle or your own collected data)\n",
    "# Assume structure: data_dir/category1/image1.jpg, data_dir/category2/image2.jpg\n",
    "DATA_DIR = 'path/to/your/waste_dataset'\n",
    "OUTPUT_DIR = 'model_output'\n",
    "TFLITE_MODEL_PATH = os.path.join(OUTPUT_DIR, 'model.tflite')\n",
    "LABELS_PATH = os.path.join(OUTPUT_DIR, 'labels.txt')\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b9e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Data Preparation ---\n",
    "\n",
    "# Option A: Using tf.keras.preprocessing.image_dataset_from_directory\n",
    "# Simple if your data is organized in folders by class\n",
    "def load_dataset_from_directory(data_dir, img_height, img_width, batch_size):\n",
    "    print(f\"Loading dataset from: {data_dir}\")\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=123,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        label_mode='int' # Use 'int' for integer labels, 'categorical' for one-hot\n",
    "    )\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=123,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        label_mode='int'\n",
    "    )\n",
    "    \n",
    "    class_names = train_ds.class_names\n",
    "    print(f\"Detected class names: {class_names}\")\n",
    "\n",
    "    # Save labels for Flutter app\n",
    "    with open(LABELS_PATH, 'w') as f:\n",
    "        for name in class_names:\n",
    "            f.write(f\"{name}\\n\")\n",
    "\n",
    "    return train_ds, val_ds, class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012599a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds, val_ds, class_names = load_dataset_from_directory(DATA_DIR, IMG_HEIGHT, IMG_WIDTH, BATCH_SIZE)\n",
    "NUM_CLASSES = len(class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0675362",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize pixel values to 0-1 range (important for TFLite conversion later)\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(preprocess_image).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(preprocess_image).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d217f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Load Gemma 3n (Visual Encoder) and Build Custom Head ---\n",
    "\n",
    "# Gemma 3n uses a vision encoder (like SigLIP or MobileNet-v5-300).\n",
    "# For direct image classification, we primarily need this visual part.\n",
    "# The Hugging Face `transformers` library provides an easy way to load pre-trained models.\n",
    "\n",
    "# Load Gemma 3n configuration (this is for the full multimodal model)\n",
    "gemma_config = GemmaConfig.from_pretrained(MODEL_NAME)\n",
    "# Note: For strict image classification, you might need to specifically extract or use\n",
    "# the vision encoder part if it's not directly exposed for a classification task.\n",
    "# Google usually provides specific examples for such tasks.\n",
    "\n",
    "# Placeholder for Gemma 3n's visual feature extractor.\n",
    "# In a real scenario, you'd load Gemma 3n's pre-trained visual encoder,\n",
    "# likely provided as a component within the overall model or a separate\n",
    "# model. For on-device, it's often a specialized MobileNet variant.\n",
    "# We'll simulate this with a pre-trained image model for demonstration,\n",
    "# as direct Keras `GemmaForImageClassification` might not be a standard API yet.\n",
    "\n",
    "# Option 1: Use a well-known image classification backbone (e.g., MobileNetV2)\n",
    "# and assume it acts as the \"visual encoder\" part, then fine-tune it.\n",
    "# This is a practical approach if Gemma 3n's exact visual encoder isn't\n",
    "# directly exposed as a separate Keras layer for classification fine-tuning.\n",
    "# However, for the challenge, you should use the *official* Gemma 3n visual encoder\n",
    "# if Google provides a clear path for it.\n",
    "\n",
    "# Let's use MobileNetV2 as a placeholder for the vision encoder for now.\n",
    "# If Google releases a specific Keras layer for Gemma 3n's vision encoder,\n",
    "# replace this with that.\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "    include_top=False, # Don't include the classification head of MobileNetV2\n",
    "    weights='imagenet' # Use pre-trained ImageNet weights\n",
    ")\n",
    "base_model.trainable = False # Freeze the base model initially\n",
    "\n",
    "inputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x) # Add a dropout layer for regularization\n",
    "outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x) # Classification head\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(), # For integer labels\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# --- 3. Train (Fine-tune) the Model ---\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds\n",
    ")\n",
    "print(\"Model training complete!\")\n",
    "\n",
    "# Save the trained Keras model\n",
    "model.save(os.path.join(OUTPUT_DIR, 'waste_classifier_model.h5'))\n",
    "print(f\"Keras model saved to {os.path.join(OUTPUT_DIR, 'waste_classifier_model.h5')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5626bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Convert to TensorFlow Lite ---\n",
    "\n",
    "print(\"\\nConverting model to TensorFlow Lite...\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# Apply optimizations for on-device deployment\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Ensure the input and output types are float32 if the model was trained with float32.\n",
    "# For integer quantization, you'd provide a representative dataset.\n",
    "converter.target_spec.supported_types = [tf.float32] \n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(TFLITE_MODEL_PATH, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"TensorFlow Lite model saved to {TFLITE_MODEL_PATH}\")\n",
    "\n",
    "# --- 5. Verify the TFLite Model (Optional) ---\n",
    "print(\"\\nVerifying TFLite model...\")\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Input details:\", input_details)\n",
    "print(\"Output details:\", output_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725158fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test with a dummy image\n",
    "dummy_image = np.random.rand(1, IMG_HEIGHT, IMG_WIDTH, 3).astype(np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], dummy_image)\n",
    "interpreter.invoke()\n",
    "tflite_output = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "print(\"TFLite output shape:\", tflite_output.shape)\n",
    "print(\"TFLite output (first 5 values):\", tflite_output[0, :5])\n",
    "\n",
    "print(\"\\nBackend model setup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
